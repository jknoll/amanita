{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Notebook: Multi-Task BEiT for Hierarchical Fungi Classification\n",
    "\n",
    "This notebook validates the trained multitask BEiT model for hierarchical fungi classification across 6 taxonomic ranks, with special focus on **Amanita phalloides** (Death Cap) recognition.\n",
    "\n",
    "## Overview\n",
    "- **Model**: BEiT multi-task model with 6 classification heads\n",
    "- **Dataset**: FungiTastic validation set (~1M samples)\n",
    "- **Ranks**: Phylum, Class, Order, Family, Genus, Species\n",
    "- **Special Focus**: Amanita phalloides (148 specimens in validation set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Image transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Display settings\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration\nCONFIG = {\n    # Paths\n    'checkpoint_path': '/home/j/Documents/git/amanita/artifacts/exp-organized-valley-fig-260101/checkpoints/AtomicDirectory_checkpoint_64/best_model.pt',\n    'val_csv_path': '/media/j/Extra FAT/FungiTastic/dataset/FungiTastic/metadata/FungiTastic/FungiTastic-ClosedSet-Val.csv',\n    'image_root': '/media/j/Extra FAT/FungiTastic/dataset/FungiTastic/FungiTastic/',\n    'taxonomic_mappings_path': '/home/j/Documents/git/amanita/taxonomic_mappings.json',\n    \n    # Model settings\n    'image_size': 224,\n    'num_classes_dict': {\n        'phylum': 7,\n        'class': 28,\n        'order': 95,\n        'family': 308,\n        'genus': 918,\n        'species': 2786\n    },\n    \n    # Dataloader settings\n    'batch_size': 64,\n    'num_workers': 8,\n    \n    # Thresholds for analysis\n    'high_confidence_threshold': 0.9,\n    'low_confidence_threshold': 0.5,\n    \n    # Output paths\n    'output_dir': '/home/j/Documents/git/amanita/validation_results',\n}\n\n# Create output directory\nos.makedirs(CONFIG['output_dir'], exist_ok=True)\n\n# Taxonomic ranks in hierarchical order\nTAXONOMIC_RANKS = ['phylum', 'class', 'order', 'family', 'genus', 'species']\n\n# Amanita phalloides info\nAMANITA_PHALLOIDES = {\n    'species_name': 'Amanita phalloides',\n    'species_id': 58,\n    'genus_id': 14,\n    'genus_name': 'Amanita',\n    'family_name': 'Amanitaceae',\n    'order_name': 'Agaricales',\n    'class_name': 'Agaricomycetes',\n    'phylum_name': 'Basidiomycota'\n}\n\nprint(\"Configuration loaded.\")\nprint(f\"\\nCheckpoint: {CONFIG['checkpoint_path']}\")\nprint(f\"Validation CSV: {CONFIG['val_csv_path']}\")\nprint(f\"Image root: {CONFIG['image_root']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load taxonomic mappings\n",
    "with open(CONFIG['taxonomic_mappings_path'], 'r') as f:\n",
    "    taxonomic_mappings = json.load(f)\n",
    "\n",
    "name_to_id = taxonomic_mappings['name_to_id']\n",
    "id_to_name = taxonomic_mappings['id_to_name']\n",
    "\n",
    "print(\"Taxonomic mappings loaded:\")\n",
    "for rank in TAXONOMIC_RANKS:\n",
    "    print(f\"  {rank:8s}: {len(name_to_id[rank]):5d} classes\")\n",
    "\n",
    "# Verify Amanita phalloides mappings\n",
    "print(f\"\\nVerifying Amanita phalloides:\")\n",
    "print(f\"  Species ID: {name_to_id['species']['Amanita phalloides']}\")\n",
    "print(f\"  Genus ID (Amanita): {name_to_id['genus']['Amanita']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Import model class and create model\n",
    "from models.beit_multitask import BEiTMultiTask, create_beit_multitask\n",
    "\n",
    "# Create model (this loads the base BEiT architecture)\n",
    "print(\"Creating BEiT multi-task model...\")\n",
    "model = create_beit_multitask(\n",
    "    pretrained=False,  # We'll load trained weights\n",
    "    num_classes_dict=CONFIG['num_classes_dict']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Load checkpoint with DDP handling\nprint(f\"Loading checkpoint from: {CONFIG['checkpoint_path']}\")\ncheckpoint = torch.load(CONFIG['checkpoint_path'], map_location='cpu', weights_only=False)\n\n# Get state dict\nstate_dict = checkpoint['model_state_dict']\n\n# Handle DDP checkpoint format (strip 'module.' prefix if present)\nif any(k.startswith('module.') for k in state_dict.keys()):\n    print(\"Removing 'module.' prefix from DDP checkpoint...\")\n    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n\n# Load weights\nmodel.load_state_dict(state_dict)\nprint(\"Weights loaded successfully!\")\n\n# Print checkpoint info if available\nif 'epoch' in checkpoint:\n    print(f\"\\nCheckpoint info:\")\n    print(f\"  Epoch: {checkpoint.get('epoch', 'N/A')}\")\n    print(f\"  Best val accuracy: {checkpoint.get('best_val_acc', 'N/A')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Move model to GPU and set to eval mode\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel ready for inference.\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2b: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6b: Load validation dataset\n",
    "from dataset_multitask import FungiTasticMultiTask\n",
    "\n",
    "# Load validation CSV\n",
    "print(f\"Loading validation data from: {CONFIG['val_csv_path']}\")\n",
    "val_df = pd.read_csv(CONFIG['val_csv_path'])\n",
    "print(f\"Loaded {len(val_df)} validation samples\")\n",
    "\n",
    "# Check for Amanita phalloides samples\n",
    "amanita_mask = val_df['species'] == 'Amanita phalloides'\n",
    "print(f\"\\nAmanita phalloides samples: {amanita_mask.sum()}\")\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "val_dataset = FungiTasticMultiTask(\n",
    "    df=val_df,\n",
    "    transform=val_transform,\n",
    "    taxonomic_mappings=taxonomic_mappings,\n",
    "    image_root=CONFIG['image_root'],\n",
    "    split='val'\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset created with {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6c: Create DataLoader with custom collate function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate batch with dictionary labels.\"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    \n",
    "    labels = {}\n",
    "    for rank in TAXONOMIC_RANKS:\n",
    "        labels[rank] = torch.tensor([item[1][rank] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    filepaths = [item[2] for item in batch]\n",
    "    \n",
    "    return images, labels, filepaths\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created with {len(val_loader)} batches\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run inference on validation set\n",
    "print(\"Running validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Storage for results\n",
    "results = {\n",
    "    'predictions': {rank: [] for rank in TAXONOMIC_RANKS},\n",
    "    'labels': {rank: [] for rank in TAXONOMIC_RANKS},\n",
    "    'confidences': {rank: [] for rank in TAXONOMIC_RANKS},\n",
    "    'top5_predictions': {rank: [] for rank in TAXONOMIC_RANKS},\n",
    "    'top5_confidences': {rank: [] for rank in TAXONOMIC_RANKS},\n",
    "    'filepaths': []\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels, filepaths) in enumerate(tqdm(val_loader, desc=\"Validating\")):\n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Process each rank\n",
    "        for rank in TAXONOMIC_RANKS:\n",
    "            logits = outputs[rank]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Top-1 predictions and confidence\n",
    "            confidence, predictions = probs.max(dim=1)\n",
    "            results['predictions'][rank].extend(predictions.cpu().numpy())\n",
    "            results['confidences'][rank].extend(confidence.cpu().numpy())\n",
    "            results['labels'][rank].extend(labels[rank].numpy())\n",
    "            \n",
    "            # Top-5 predictions and confidences\n",
    "            top5_conf, top5_pred = probs.topk(5, dim=1)\n",
    "            results['top5_predictions'][rank].extend(top5_pred.cpu().numpy())\n",
    "            results['top5_confidences'][rank].extend(top5_conf.cpu().numpy())\n",
    "        \n",
    "        # Store filepaths\n",
    "        results['filepaths'].extend(filepaths)\n",
    "\n",
    "print(f\"\\nValidation complete!\")\n",
    "print(f\"Total samples processed: {len(results['filepaths'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Convert results to numpy arrays\n",
    "for rank in TAXONOMIC_RANKS:\n",
    "    results['predictions'][rank] = np.array(results['predictions'][rank])\n",
    "    results['labels'][rank] = np.array(results['labels'][rank])\n",
    "    results['confidences'][rank] = np.array(results['confidences'][rank])\n",
    "    results['top5_predictions'][rank] = np.array(results['top5_predictions'][rank])\n",
    "    results['top5_confidences'][rank] = np.array(results['top5_confidences'][rank])\n",
    "\n",
    "print(\"Results converted to numpy arrays.\")\n",
    "print(f\"\\nShape verification:\")\n",
    "for rank in TAXONOMIC_RANKS:\n",
    "    print(f\"  {rank:8s}: predictions={results['predictions'][rank].shape}, top5={results['top5_predictions'][rank].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Calculate per-rank metrics\n",
    "metrics = {}\n",
    "\n",
    "for rank in TAXONOMIC_RANKS:\n",
    "    preds = results['predictions'][rank]\n",
    "    lbls = results['labels'][rank]\n",
    "    confs = results['confidences'][rank]\n",
    "    top5_preds = results['top5_predictions'][rank]\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics[rank] = {\n",
    "        'accuracy': accuracy_score(lbls, preds),\n",
    "        'precision': precision_score(lbls, preds, average='macro', zero_division=0),\n",
    "        'recall': recall_score(lbls, preds, average='macro', zero_division=0),\n",
    "        'f1': f1_score(lbls, preds, average='macro', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Top-5 accuracy\n",
    "    top5_correct = np.any(top5_preds == lbls.reshape(-1, 1), axis=1)\n",
    "    metrics[rank]['top5_accuracy'] = top5_correct.mean()\n",
    "    \n",
    "    # Confidence statistics\n",
    "    correct_mask = preds == lbls\n",
    "    metrics[rank]['avg_confidence'] = confs.mean()\n",
    "    metrics[rank]['avg_confidence_correct'] = confs[correct_mask].mean() if correct_mask.sum() > 0 else 0\n",
    "    metrics[rank]['avg_confidence_incorrect'] = confs[~correct_mask].mean() if (~correct_mask).sum() > 0 else 0\n",
    "    metrics[rank]['confidence_gap'] = metrics[rank]['avg_confidence_correct'] - metrics[rank]['avg_confidence_incorrect']\n",
    "\n",
    "# Calculate hierarchical accuracy (all ranks correct)\n",
    "all_correct = np.ones(len(results['filepaths']), dtype=bool)\n",
    "for rank in TAXONOMIC_RANKS:\n",
    "    all_correct &= (results['predictions'][rank] == results['labels'][rank])\n",
    "metrics['hierarchical_accuracy'] = all_correct.mean()\n",
    "\n",
    "print(\"Metrics calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Display metrics table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION METRICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create metrics dataframe\n",
    "metrics_data = []\n",
    "for rank in TAXONOMIC_RANKS:\n",
    "    m = metrics[rank]\n",
    "    metrics_data.append({\n",
    "        'Rank': rank.capitalize(),\n",
    "        'Top-1 Acc': f\"{m['accuracy']:.2%}\",\n",
    "        'Top-5 Acc': f\"{m['top5_accuracy']:.2%}\",\n",
    "        'Precision': f\"{m['precision']:.2%}\",\n",
    "        'Recall': f\"{m['recall']:.2%}\",\n",
    "        'F1 (Macro)': f\"{m['f1']:.2%}\",\n",
    "        'Avg Conf': f\"{m['avg_confidence']:.3f}\",\n",
    "        'Conf Gap': f\"{m['confidence_gap']:.3f}\"\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nHierarchical Accuracy (all ranks correct): {metrics['hierarchical_accuracy']:.2%}\")\n",
    "\n",
    "# Average metrics\n",
    "avg_acc = np.mean([metrics[r]['accuracy'] for r in TAXONOMIC_RANKS])\n",
    "avg_f1 = np.mean([metrics[r]['f1'] for r in TAXONOMIC_RANKS])\n",
    "print(f\"Average Accuracy across ranks: {avg_acc:.2%}\")\n",
    "print(f\"Average F1 across ranks: {avg_f1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: Plot confidence distributions\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor idx, rank in enumerate(TAXONOMIC_RANKS):\n    ax = axes[idx]\n    \n    preds = results['predictions'][rank]\n    lbls = results['labels'][rank]\n    confs = results['confidences'][rank]\n    \n    correct_mask = preds == lbls\n    \n    # Plot distributions with error handling for edge cases\n    try:\n        if correct_mask.sum() > 0:\n            ax.hist(confs[correct_mask], bins=50, alpha=0.6, label='Correct', color='green', density=True)\n        if (~correct_mask).sum() > 0:\n            ax.hist(confs[~correct_mask], bins=50, alpha=0.6, label='Incorrect', color='red', density=True)\n    except ValueError:\n        # Fall back to fewer bins if data range is too small\n        if correct_mask.sum() > 0:\n            ax.hist(confs[correct_mask], bins='auto', alpha=0.6, label='Correct', color='green', density=True)\n        if (~correct_mask).sum() > 0:\n            ax.hist(confs[~correct_mask], bins='auto', alpha=0.6, label='Incorrect', color='red', density=True)\n    \n    ax.set_xlabel('Confidence')\n    ax.set_ylabel('Density')\n    ax.set_title(f'{rank.capitalize()}\\nAcc: {metrics[rank][\"accuracy\"]:.2%}')\n    ax.legend()\n    ax.set_xlim(0, 1)\n\nplt.suptitle('Confidence Distributions: Correct vs Incorrect Predictions', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.savefig(os.path.join(CONFIG['output_dir'], 'confidence_distributions.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"Saved: {CONFIG['output_dir']}/confidence_distributions.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Confusion matrix for Phylum (7 classes)\n",
    "rank = 'phylum'\n",
    "preds = results['predictions'][rank]\n",
    "lbls = results['labels'][rank]\n",
    "\n",
    "cm = confusion_matrix(lbls, preds)\n",
    "\n",
    "# Get class names\n",
    "class_names = [id_to_name[rank][str(i)] for i in range(CONFIG['num_classes_dict'][rank])]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix - Phylum (Acc: {metrics[rank][\"accuracy\"]:.2%})')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'confusion_matrix_phylum.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {CONFIG['output_dir']}/confusion_matrix_phylum.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Confusion matrix for Class (28 classes)\n",
    "rank = 'class'\n",
    "preds = results['predictions'][rank]\n",
    "lbls = results['labels'][rank]\n",
    "\n",
    "cm = confusion_matrix(lbls, preds)\n",
    "\n",
    "# Get class names\n",
    "class_names = [id_to_name[rank][str(i)] for i in range(CONFIG['num_classes_dict'][rank])]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=False, cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix - Class (Acc: {metrics[rank][\"accuracy\"]:.2%})')\n",
    "plt.xticks(rotation=90, ha='center', fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'confusion_matrix_class.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved: {CONFIG['output_dir']}/confusion_matrix_class.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Top confused pairs for higher ranks (Order, Family, Genus, Species)\n",
    "def get_top_confused_pairs(predictions, labels, id_to_name_dict, top_n=15):\n",
    "    \"\"\"Get top-N most confused class pairs.\"\"\"\n",
    "    confusion_counts = defaultdict(int)\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        if pred != label:\n",
    "            confusion_counts[(label, pred)] += 1\n",
    "    \n",
    "    # Sort by count\n",
    "    sorted_pairs = sorted(confusion_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    # Convert to readable format\n",
    "    results = []\n",
    "    for (true_id, pred_id), count in sorted_pairs:\n",
    "        true_name = id_to_name_dict[str(true_id)]\n",
    "        pred_name = id_to_name_dict[str(pred_id)]\n",
    "        results.append({\n",
    "            'True': true_name,\n",
    "            'Predicted': pred_name,\n",
    "            'Count': count\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Top-15 Confused Pairs by Rank\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for rank in ['order', 'family', 'genus', 'species']:\n",
    "    print(f\"\\n{rank.upper()}:\")\n",
    "    confused_df = get_top_confused_pairs(\n",
    "        results['predictions'][rank],\n",
    "        results['labels'][rank],\n",
    "        id_to_name[rank]\n",
    "    )\n",
    "    print(confused_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Well vs Poorly Recognized Specimens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 15: Identify well and poorly recognized specimens (at species level)\nrank = 'species'\npreds = results['predictions'][rank]\nlbls = results['labels'][rank]\nconfs = results['confidences'][rank]\n\ncorrect_mask = preds == lbls\n\n# Well-recognized: correct prediction + high confidence\nwell_recognized_mask = correct_mask & (confs >= CONFIG['high_confidence_threshold'])\n\n# Poorly-recognized: incorrect OR low confidence\npoorly_recognized_mask = ~correct_mask | (confs < CONFIG['low_confidence_threshold'])\n\nprint(f\"Well-recognized specimens (correct + conf >= {CONFIG['high_confidence_threshold']}): {well_recognized_mask.sum():,}\")\nprint(f\"Poorly-recognized specimens (incorrect OR conf < {CONFIG['low_confidence_threshold']}): {poorly_recognized_mask.sum():,}\")\n\n# Get indices\nwell_recognized_indices = np.where(well_recognized_mask)[0]\npoorly_recognized_indices = np.where(poorly_recognized_mask & ~correct_mask)[0]  # Focus on incorrect ones\n\n\n# Helper function to filter out microscopy/spore images\ndef is_microscopy_image(filepath, caption=None):\n    \"\"\"\n    Detect if an image is likely a microscopy/spore image rather than a macroscopic photo.\n    \n    Uses multiple heuristics:\n    1. Caption-based detection (if captions available)\n    2. Filename patterns\n    \n    Returns True if image should be excluded.\n    \"\"\"\n    # Check caption for microscopy keywords\n    if caption:\n        caption_lower = caption.lower()\n        microscopy_keywords = [\n            'spore', 'microscop', 'magnif', 'slide', 'cell', 'hypha', 'hyphae',\n            'basidi', 'ascus', 'asci', 'cystid', 'gill section', 'cross section',\n            'Î¼m', 'micron', '400x', '1000x', 'oil immersion'\n        ]\n        for keyword in microscopy_keywords:\n            if keyword in caption_lower:\n                return True\n    \n    # Check filename patterns (some datasets use prefixes for microscopy)\n    filename = os.path.basename(filepath).lower()\n    micro_patterns = ['micro', 'spore', 'slide', 'section']\n    for pattern in micro_patterns:\n        if pattern in filename:\n            return True\n    \n    return False\n\n\ndef filter_non_microscopy_indices(indices, filepaths, val_df=None):\n    \"\"\"\n    Filter indices to exclude microscopy images.\n    \n    Args:\n        indices: Array of indices to filter\n        filepaths: List of all filepaths (indexed by results indices)\n        val_df: Optional DataFrame with captions column\n    \n    Returns:\n        Filtered array of indices\n    \"\"\"\n    filtered = []\n    for idx in indices:\n        filepath = filepaths[idx]\n        \n        # Try to get caption if available\n        caption = None\n        if val_df is not None and 'captions' in val_df.columns:\n            try:\n                # Find matching row by filename\n                filename = os.path.basename(filepath)\n                matching = val_df[val_df['filename'] == filename]\n                if len(matching) > 0:\n                    caption = matching.iloc[0]['captions']\n            except Exception:\n                pass\n        \n        if not is_microscopy_image(filepath, caption):\n            filtered.append(idx)\n    \n    return np.array(filtered)\n\n\n# Filter indices to exclude microscopy images\nprint(\"\\nFiltering out potential microscopy/spore images...\")\ntry:\n    well_recognized_indices_filtered = filter_non_microscopy_indices(\n        well_recognized_indices, results['filepaths'], val_df\n    )\n    poorly_recognized_indices_filtered = filter_non_microscopy_indices(\n        poorly_recognized_indices, results['filepaths'], val_df\n    )\n    print(f\"Well-recognized after filtering: {len(well_recognized_indices_filtered):,} (removed {len(well_recognized_indices) - len(well_recognized_indices_filtered)})\")\n    print(f\"Poorly-recognized after filtering: {len(poorly_recognized_indices_filtered):,} (removed {len(poorly_recognized_indices) - len(poorly_recognized_indices_filtered)})\")\n    \n    # Use filtered indices\n    well_recognized_indices = well_recognized_indices_filtered\n    poorly_recognized_indices = poorly_recognized_indices_filtered\nexcept Exception as e:\n    print(f\"Warning: Could not filter microscopy images: {e}\")\n    print(\"Using unfiltered indices.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Display well-recognized specimens\n",
    "def display_specimens(indices, title, n_samples=12, n_cols=4):\n",
    "    \"\"\"Display a grid of specimen images with predictions.\"\"\"\n",
    "    if len(indices) == 0:\n",
    "        print(f\"No specimens found for: {title}\")\n",
    "        return None\n",
    "    \n",
    "    # Sample indices if too many\n",
    "    if len(indices) > n_samples:\n",
    "        sample_indices = np.random.choice(indices, n_samples, replace=False)\n",
    "    else:\n",
    "        sample_indices = indices[:n_samples]\n",
    "    \n",
    "    n_rows = (len(sample_indices) + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "    axes = np.array(axes).flatten()\n",
    "    \n",
    "    for ax_idx, sample_idx in enumerate(sample_indices):\n",
    "        ax = axes[ax_idx]\n",
    "        \n",
    "        # Load and display image\n",
    "        filepath = results['filepaths'][sample_idx]\n",
    "        try:\n",
    "            img = Image.open(filepath)\n",
    "            ax.imshow(img)\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, 'Image\\nNot Found', ha='center', va='center')\n",
    "        \n",
    "        # Get prediction info\n",
    "        pred_id = results['predictions']['species'][sample_idx]\n",
    "        true_id = results['labels']['species'][sample_idx]\n",
    "        conf = results['confidences']['species'][sample_idx]\n",
    "        \n",
    "        pred_name = id_to_name['species'][str(pred_id)]\n",
    "        true_name = id_to_name['species'][str(true_id)]\n",
    "        \n",
    "        is_correct = pred_id == true_id\n",
    "        color = 'green' if is_correct else 'red'\n",
    "        \n",
    "        ax.set_title(f\"True: {true_name[:25]}\\nPred: {pred_name[:25]}\\nConf: {conf:.3f}\",\n",
    "                     fontsize=8, color=color)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide empty axes\n",
    "    for ax_idx in range(len(sample_indices), len(axes)):\n",
    "        axes[ax_idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Display well-recognized specimens\n",
    "fig = display_specimens(well_recognized_indices, 'Well-Recognized Specimens (High Confidence Correct)')\n",
    "if fig:\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'well_recognized_specimens.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {CONFIG['output_dir']}/well_recognized_specimens.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Display poorly-recognized specimens\n",
    "fig = display_specimens(poorly_recognized_indices, 'Poorly-Recognized Specimens (Incorrect Predictions)')\n",
    "if fig:\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'poorly_recognized_specimens.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {CONFIG['output_dir']}/poorly_recognized_specimens.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Single Observation Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Single observation validation function\n",
    "def validate_single_observation(idx_or_path, show_image=True):\n",
    "    \"\"\"\n",
    "    Validate a single observation and show top-5 predictions at all ranks.\n",
    "    \n",
    "    Args:\n",
    "        idx_or_path: Either an index into the validation set or a filepath string\n",
    "        show_image: Whether to display the image\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results for all ranks\n",
    "    \"\"\"\n",
    "    # Determine if we're using pre-computed results or need to run inference\n",
    "    if isinstance(idx_or_path, int):\n",
    "        idx = idx_or_path\n",
    "        filepath = results['filepaths'][idx]\n",
    "        use_precomputed = True\n",
    "    else:\n",
    "        filepath = idx_or_path\n",
    "        use_precomputed = False\n",
    "        # Find index if filepath exists in results\n",
    "        if filepath in results['filepaths']:\n",
    "            idx = results['filepaths'].index(filepath)\n",
    "            use_precomputed = True\n",
    "    \n",
    "    print(f\"Validating: {filepath}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display image\n",
    "    if show_image:\n",
    "        try:\n",
    "            img = Image.open(filepath)\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(os.path.basename(filepath))\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display image: {e}\")\n",
    "    \n",
    "    # Get results\n",
    "    observation_results = {}\n",
    "    \n",
    "    print(f\"\\n{'Rank':<10} {'True':<30} {'Predicted':<30} {'Conf':<8} {'Correct'}\")\n",
    "    print(\"-\"*90)\n",
    "    \n",
    "    for rank in TAXONOMIC_RANKS:\n",
    "        if use_precomputed:\n",
    "            true_id = results['labels'][rank][idx]\n",
    "            pred_id = results['predictions'][rank][idx]\n",
    "            conf = results['confidences'][rank][idx]\n",
    "            top5_preds = results['top5_predictions'][rank][idx]\n",
    "            top5_confs = results['top5_confidences'][rank][idx]\n",
    "        else:\n",
    "            # Would need to run inference - not implemented for external files\n",
    "            print(f\"External file inference not implemented. Use validation set index.\")\n",
    "            return None\n",
    "        \n",
    "        true_name = id_to_name[rank][str(true_id)]\n",
    "        pred_name = id_to_name[rank][str(pred_id)]\n",
    "        is_correct = true_id == pred_id\n",
    "        \n",
    "        print(f\"{rank.capitalize():<10} {true_name:<30} {pred_name:<30} {conf:.4f}  {'YES' if is_correct else 'NO'}\")\n",
    "        \n",
    "        observation_results[rank] = {\n",
    "            'true_id': int(true_id),\n",
    "            'true_name': true_name,\n",
    "            'pred_id': int(pred_id),\n",
    "            'pred_name': pred_name,\n",
    "            'confidence': float(conf),\n",
    "            'correct': is_correct,\n",
    "            'top5': [(id_to_name[rank][str(p)], float(c)) for p, c in zip(top5_preds, top5_confs)]\n",
    "        }\n",
    "    \n",
    "    # Show top-5 predictions for species\n",
    "    print(f\"\\nTop-5 Species Predictions:\")\n",
    "    for i, (name, conf) in enumerate(observation_results['species']['top5']):\n",
    "        marker = '*' if name == observation_results['species']['true_name'] else ' '\n",
    "        print(f\"  {i+1}. {name:<40} {conf:.4f} {marker}\")\n",
    "    \n",
    "    return observation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Example - validate a random sample\n",
    "random_idx = np.random.randint(0, len(results['filepaths']))\n",
    "example_results = validate_single_observation(random_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Amanita Phalloides Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Filter Amanita phalloides specimens\n",
    "amanita_species_id = name_to_id['species']['Amanita phalloides']\n",
    "amanita_mask = results['labels']['species'] == amanita_species_id\n",
    "amanita_indices = np.where(amanita_mask)[0]\n",
    "\n",
    "print(f\"Amanita phalloides (Death Cap) Analysis\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total specimens in validation set: {len(amanita_indices)}\")\n",
    "print(f\"Species ID: {amanita_species_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Species-level analysis for Amanita phalloides\n",
    "amanita_preds = results['predictions']['species'][amanita_mask]\n",
    "amanita_confs = results['confidences']['species'][amanita_mask]\n",
    "\n",
    "# Accuracy at species level\n",
    "species_correct = amanita_preds == amanita_species_id\n",
    "species_accuracy = species_correct.mean()\n",
    "\n",
    "print(f\"\\nSpecies-Level Performance:\")\n",
    "print(f\"  Correctly identified: {species_correct.sum()} / {len(amanita_indices)} ({species_accuracy:.2%})\")\n",
    "print(f\"  Average confidence when correct: {amanita_confs[species_correct].mean():.4f}\" if species_correct.sum() > 0 else \"  N/A\")\n",
    "print(f\"  Average confidence when incorrect: {amanita_confs[~species_correct].mean():.4f}\" if (~species_correct).sum() > 0 else \"  N/A\")\n",
    "\n",
    "# What is it confused with?\n",
    "if (~species_correct).sum() > 0:\n",
    "    print(f\"\\nMisclassified as (species level):\")\n",
    "    confused_species = pd.Series(amanita_preds[~species_correct]).value_counts().head(10)\n",
    "    for pred_id, count in confused_species.items():\n",
    "        pred_name = id_to_name['species'][str(pred_id)]\n",
    "        print(f\"  {pred_name:<40}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Hierarchical analysis - accuracy at each rank\n",
    "print(f\"\\nHierarchical Accuracy for Amanita phalloides:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "amanita_hierarchical = {}\n",
    "for rank in TAXONOMIC_RANKS:\n",
    "    true_name = AMANITA_PHALLOIDES.get(f'{rank}_name', 'Amanita phalloides' if rank == 'species' else None)\n",
    "    if true_name is None:\n",
    "        continue\n",
    "    \n",
    "    true_id = name_to_id[rank][true_name]\n",
    "    rank_preds = results['predictions'][rank][amanita_mask]\n",
    "    rank_correct = rank_preds == true_id\n",
    "    rank_accuracy = rank_correct.mean()\n",
    "    \n",
    "    amanita_hierarchical[rank] = {\n",
    "        'true_name': true_name,\n",
    "        'true_id': true_id,\n",
    "        'correct': rank_correct.sum(),\n",
    "        'total': len(rank_correct),\n",
    "        'accuracy': rank_accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"  {rank.capitalize():8s}: {rank_correct.sum():4d}/{len(rank_correct):4d} ({rank_accuracy:.2%}) - {true_name}\")\n",
    "\n",
    "# Key finding: Can we at least identify it as Amanita genus?\n",
    "genus_correct = amanita_hierarchical['genus']['accuracy']\n",
    "species_correct_pct = amanita_hierarchical['species']['accuracy']\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"KEY FINDING: Amanita phalloides Recognition\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Species-level accuracy: {species_correct_pct:.2%}\")\n",
    "print(f\"Genus-level accuracy (Amanita): {genus_correct:.2%}\")\n",
    "\n",
    "if genus_correct > species_correct_pct:\n",
    "    print(f\"\\n> Even when species is wrong, {genus_correct - species_correct_pct:.2%} more are correctly\")\n",
    "    print(f\"  identified at the genus level (Amanita).\")\n",
    "    print(f\"  This is valuable for poisonous mushroom detection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Display Amanita phalloides specimens\n",
    "# Show both correctly and incorrectly classified specimens\n",
    "\n",
    "species_correct_mask = results['predictions']['species'][amanita_mask] == amanita_species_id\n",
    "correct_amanita_indices = amanita_indices[species_correct_mask]\n",
    "incorrect_amanita_indices = amanita_indices[~species_correct_mask]\n",
    "\n",
    "print(f\"Correctly classified Amanita phalloides: {len(correct_amanita_indices)}\")\n",
    "print(f\"Incorrectly classified Amanita phalloides: {len(incorrect_amanita_indices)}\")\n",
    "\n",
    "# Display correctly classified ones\n",
    "if len(correct_amanita_indices) > 0:\n",
    "    fig = display_specimens(correct_amanita_indices, \n",
    "                           f'Correctly Identified Amanita phalloides ({len(correct_amanita_indices)} specimens)',\n",
    "                           n_samples=8, n_cols=4)\n",
    "    if fig:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Display misclassified Amanita phalloides with analysis\n",
    "if len(incorrect_amanita_indices) > 0:\n",
    "    print(f\"\\nMisclassified Amanita phalloides specimens:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show detailed predictions for misclassified ones\n",
    "    for idx in incorrect_amanita_indices[:5]:  # Show first 5\n",
    "        print(f\"\\n--- Specimen at index {idx} ---\")\n",
    "        _ = validate_single_observation(idx, show_image=True)\n",
    "    \n",
    "    # Summary figure\n",
    "    fig = display_specimens(incorrect_amanita_indices,\n",
    "                           f'Misclassified Amanita phalloides ({len(incorrect_amanita_indices)} specimens)',\n",
    "                           n_samples=8, n_cols=4)\n",
    "    if fig:\n",
    "        plt.savefig(os.path.join(CONFIG['output_dir'], 'amanita_phalloides_analysis.png'), \n",
    "                    dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"\\nSaved: {CONFIG['output_dir']}/amanita_phalloides_analysis.png\")\n",
    "else:\n",
    "    print(\"\\nAll Amanita phalloides specimens were correctly classified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: HTML Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Generate HTML report\n",
    "def generate_html_report(metrics, results, amanita_hierarchical, output_path):\n",
    "    \"\"\"Generate a comprehensive HTML validation report.\"\"\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Validation Report - Multi-Task BEiT Fungi Classification</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }}\n",
    "        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}\n",
    "        h2 {{ color: #34495e; margin-top: 30px; }}\n",
    "        h3 {{ color: #7f8c8d; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; background: white; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
    "        th {{ background-color: #3498db; color: white; }}\n",
    "        tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
    "        tr:hover {{ background-color: #f1f1f1; }}\n",
    "        .metric-good {{ color: #27ae60; font-weight: bold; }}\n",
    "        .metric-medium {{ color: #f39c12; font-weight: bold; }}\n",
    "        .metric-poor {{ color: #e74c3c; font-weight: bold; }}\n",
    "        .key-finding {{ background-color: #e8f6f3; border-left: 4px solid #1abc9c; padding: 15px; margin: 20px 0; }}\n",
    "        .warning {{ background-color: #fdf2e9; border-left: 4px solid #e67e22; padding: 15px; margin: 20px 0; }}\n",
    "        .image-container {{ text-align: center; margin: 20px 0; }}\n",
    "        img {{ max-width: 100%; height: auto; border: 1px solid #ddd; }}\n",
    "        .summary-box {{ background: white; padding: 20px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); margin: 20px 0; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Validation Report: Multi-Task BEiT for Hierarchical Fungi Classification</h1>\n",
    "    \n",
    "    <div class=\"summary-box\">\n",
    "        <h2>Summary</h2>\n",
    "        <p><strong>Model:</strong> BEiT Multi-Task (6 classification heads)</p>\n",
    "        <p><strong>Validation Samples:</strong> {len(results['filepaths']):,}</p>\n",
    "        <p><strong>Hierarchical Accuracy:</strong> <span class=\"{'metric-good' if metrics['hierarchical_accuracy'] > 0.5 else 'metric-medium' if metrics['hierarchical_accuracy'] > 0.3 else 'metric-poor'}\">{metrics['hierarchical_accuracy']:.2%}</span></p>\n",
    "    </div>\n",
    "    \n",
    "    <h2>Per-Rank Metrics</h2>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Rank</th>\n",
    "            <th>Top-1 Accuracy</th>\n",
    "            <th>Top-5 Accuracy</th>\n",
    "            <th>Precision</th>\n",
    "            <th>Recall</th>\n",
    "            <th>F1 (Macro)</th>\n",
    "            <th>Avg Confidence</th>\n",
    "        </tr>\n",
    "\"\"\"\n",
    "    \n",
    "    for rank in TAXONOMIC_RANKS:\n",
    "        m = metrics[rank]\n",
    "        acc_class = 'metric-good' if m['accuracy'] > 0.8 else 'metric-medium' if m['accuracy'] > 0.5 else 'metric-poor'\n",
    "        html += f\"\"\"\n",
    "        <tr>\n",
    "            <td><strong>{rank.capitalize()}</strong></td>\n",
    "            <td class=\"{acc_class}\">{m['accuracy']:.2%}</td>\n",
    "            <td>{m['top5_accuracy']:.2%}</td>\n",
    "            <td>{m['precision']:.2%}</td>\n",
    "            <td>{m['recall']:.2%}</td>\n",
    "            <td>{m['f1']:.2%}</td>\n",
    "            <td>{m['avg_confidence']:.3f}</td>\n",
    "        </tr>\n",
    "\"\"\"\n",
    "    \n",
    "    html += \"\"\"    </table>\n",
    "    \n",
    "    <h2>Confidence Distributions</h2>\n",
    "    <div class=\"image-container\">\n",
    "        <img src=\"confidence_distributions.png\" alt=\"Confidence Distributions\">\n",
    "    </div>\n",
    "    \n",
    "    <h2>Confusion Matrices</h2>\n",
    "    <h3>Phylum (7 classes)</h3>\n",
    "    <div class=\"image-container\">\n",
    "        <img src=\"confusion_matrix_phylum.png\" alt=\"Confusion Matrix - Phylum\">\n",
    "    </div>\n",
    "    \n",
    "    <h3>Class (28 classes)</h3>\n",
    "    <div class=\"image-container\">\n",
    "        <img src=\"confusion_matrix_class.png\" alt=\"Confusion Matrix - Class\">\n",
    "    </div>\n",
    "    \n",
    "    <h2>Amanita phalloides (Death Cap) Analysis</h2>\n",
    "    <div class=\"warning\">\n",
    "        <strong>Important:</strong> Amanita phalloides is one of the most poisonous mushrooms. \n",
    "        Accurate identification is critical for public safety.\n",
    "    </div>\n",
    "    \n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Taxonomic Rank</th>\n",
    "            <th>Expected Value</th>\n",
    "            <th>Correct</th>\n",
    "            <th>Total</th>\n",
    "            <th>Accuracy</th>\n",
    "        </tr>\n",
    "\"\"\"\n",
    "    \n",
    "    for rank in TAXONOMIC_RANKS:\n",
    "        if rank in amanita_hierarchical:\n",
    "            h = amanita_hierarchical[rank]\n",
    "            acc_class = 'metric-good' if h['accuracy'] > 0.8 else 'metric-medium' if h['accuracy'] > 0.5 else 'metric-poor'\n",
    "            html += f\"\"\"\n",
    "        <tr>\n",
    "            <td><strong>{rank.capitalize()}</strong></td>\n",
    "            <td>{h['true_name']}</td>\n",
    "            <td>{h['correct']}</td>\n",
    "            <td>{h['total']}</td>\n",
    "            <td class=\"{acc_class}\">{h['accuracy']:.2%}</td>\n",
    "        </tr>\n",
    "\"\"\"\n",
    "    \n",
    "    # Key findings\n",
    "    species_acc = amanita_hierarchical.get('species', {}).get('accuracy', 0)\n",
    "    genus_acc = amanita_hierarchical.get('genus', {}).get('accuracy', 0)\n",
    "    \n",
    "    html += f\"\"\"\n",
    "    </table>\n",
    "    \n",
    "    <div class=\"key-finding\">\n",
    "        <h3>Key Finding</h3>\n",
    "        <p><strong>Species-level recognition:</strong> {species_acc:.2%}</p>\n",
    "        <p><strong>Genus-level recognition (Amanita):</strong> {genus_acc:.2%}</p>\n",
    "        <p>Even when the exact species is misidentified, the model correctly identifies \n",
    "           the genus as Amanita in {genus_acc:.2%} of cases, which is valuable for \n",
    "           flagging potentially dangerous specimens.</p>\n",
    "    </div>\n",
    "    \n",
    "    <h2>Specimen Examples</h2>\n",
    "    <h3>Well-Recognized Specimens</h3>\n",
    "    <div class=\"image-container\">\n",
    "        <img src=\"well_recognized_specimens.png\" alt=\"Well-Recognized Specimens\">\n",
    "    </div>\n",
    "    \n",
    "    <h3>Poorly-Recognized Specimens</h3>\n",
    "    <div class=\"image-container\">\n",
    "        <img src=\"poorly_recognized_specimens.png\" alt=\"Poorly-Recognized Specimens\">\n",
    "    </div>\n",
    "    \n",
    "    <footer style=\"margin-top: 40px; padding-top: 20px; border-top: 1px solid #ddd; color: #7f8c8d;\">\n",
    "        <p>Generated by validation_notebook.ipynb</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate report\n",
    "report_path = os.path.join(CONFIG['output_dir'], 'validation_report.html')\n",
    "generate_html_report(metrics, results, amanita_hierarchical, report_path)\n",
    "print(f\"HTML report generated: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Save results to pickle for future analysis\n",
    "save_data = {\n",
    "    'metrics': metrics,\n",
    "    'results': results,\n",
    "    'amanita_hierarchical': amanita_hierarchical,\n",
    "    'config': CONFIG,\n",
    "    'taxonomic_mappings': taxonomic_mappings\n",
    "}\n",
    "\n",
    "pickle_path = os.path.join(CONFIG['output_dir'], 'validation_results.pkl')\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(f\"Results saved to: {pickle_path}\")\n",
    "\n",
    "# Summary of output files\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"OUTPUT FILES GENERATED:\")\n",
    "print(\"=\"*60)\n",
    "for f in os.listdir(CONFIG['output_dir']):\n",
    "    filepath = os.path.join(CONFIG['output_dir'], f)\n",
    "    size = os.path.getsize(filepath) / 1024\n",
    "    print(f\"  {f:<40} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27: Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"-\"*40)\n",
    "for rank in TAXONOMIC_RANKS:\n",
    "    print(f\"  {rank.capitalize():8s}: {metrics[rank]['accuracy']:.2%} (Top-5: {metrics[rank]['top5_accuracy']:.2%})\")\n",
    "\n",
    "print(f\"\\nHierarchical Accuracy (all ranks correct): {metrics['hierarchical_accuracy']:.2%}\")\n",
    "\n",
    "print(f\"\\nAmanita phalloides Recognition:\")\n",
    "print(f\"-\"*40)\n",
    "print(f\"  Species-level: {amanita_hierarchical['species']['accuracy']:.2%}\")\n",
    "print(f\"  Genus-level:   {amanita_hierarchical['genus']['accuracy']:.2%}\")\n",
    "\n",
    "print(f\"\\nOutput files saved to: {CONFIG['output_dir']}\")\n",
    "print(f\"Open {CONFIG['output_dir']}/validation_report.html in a browser to view the full report.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}