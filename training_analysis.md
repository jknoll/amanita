# Multi-Task BEiT Training Analysis

## Epoch Accounting Investigation

### Key Finding: AtomicDirectory Numbers ‚â† Epoch Numbers ‚úÖ

**AtomicDirectory checkpoint numbers** are auto-incrementing counters managed by the ISC platform across ALL checkpoint save operations, including:
- Different experiment runs
- Test checkpoints
- Regular epoch checkpoints
- Failed attempts

**Actual epoch numbers** are stored inside the checkpoint files.

### Evidence:

| Checkpoint Directory | Actual Epoch | Experiment |
|---------------------|--------------|------------|
| `AtomicDirectory_checkpoint_26` | 4 | exp-heliotrope-leeward-fish-251229 |
| `AtomicDirectory_checkpoint_4` | 2 | exp-glowing-rustic-giraffatitan-251230 |
| `AtomicDirectory_checkpoint_84` | 7 | exp-coherent-kind-character-251230 |

### Epoch Resumption Logic (Verified Correct):

```python
# In load_checkpoint():
start_epoch = checkpoint['epoch'] + 1  # e.g., 7 + 1 = 8

# In training loop:
for epoch in range(start_epoch, args.epochs + 1):  # range(8, 11) = [8, 9, 10]
```

**Conclusion**: Epoch accounting is working correctly. The training script properly resumes from the next epoch after the checkpoint.

---

## Training Progress Summary

### Best Training Run (exp-coherent-kind-character-251230)

| Epoch | Train Loss | Train Acc | Train F1 | Val Loss | Val Acc | Val F1 | Best Model |
|-------|-----------|-----------|----------|----------|---------|--------|------------|
| 1 | 1.4403 | 64.86% | 34.04% | 1.1696 | 72.36% | 47.13% | ‚òÖ |
| 2 | 0.5721 | 79.98% | 61.54% | 1.1240 | 76.10% | 53.14% | ‚òÖ |
| 3 | 0.3204 | 84.92% | 74.21% | 1.1881 | 77.95% | 54.05% | ‚òÖ |
| 4 | 0.2140 | 87.53% | 80.54% | 1.2047 | 77.87% | 50.73% | - |
| 5 | 0.1684 | 88.75% | 82.91% | 1.3734 | 78.96% | 55.99% | ‚òÖ |
| 6 | 0.1346 | 89.92% | 82.63% | 1.3849 | 78.06% | 53.17% | - |
| 7 | 0.1205 | 90.93% | 85.54% | (interrupted) | - | - | - |

### Performance Metrics

**Best Validation F1**: 55.99% (Epoch 5)

**Improvement vs. Baseline**:
- First single-epoch run: ~44% Val F1
- Best multi-task run: ~56% Val F1
- **Improvement: +12 percentage points** (27% relative improvement)

### Training Dynamics

**Training Progress**:
- Train Loss: 1.44 ‚Üí 0.12 (91% reduction)
- Train Accuracy: 64.9% ‚Üí 90.9% (+26 pp)
- Train F1: 34.0% ‚Üí 85.5% (+51.5 pp)

**Validation Performance**:
- Val Loss: 1.17 ‚Üí 1.38 (increasing - overfitting signal)
- Val Accuracy: 72.4% ‚Üí ~78% (plateaus around 77-79%)
- Val F1: 47.1% ‚Üí 56.0% (peaks at epoch 5)

---

## Observations

### 1. Training Quality ‚úÖ

- **Strong convergence**: Training loss decreases consistently
- **Good learning**: F1 score improves significantly across all taxonomic ranks
- **Multi-task learning working**: The model successfully learns hierarchical taxonomy

### 2. Overfitting Indicators ‚ö†Ô∏è

- **Val loss increases** after epoch 3 (1.19 ‚Üí 1.38)
- **Val F1 plateaus** around 54-56% while train F1 continues to 85%
- **Train-val gap**: By epoch 6, train F1 (82.6%) >> val F1 (53.2%)

**Potential causes**:
- Model capacity (87.9M parameters) may be too large for the task
- May need stronger regularization (dropout, weight decay)
- Data augmentation could help generalization

### 3. Training Interruptions üîÑ

The logs show **multiple restarts at epoch 1**, suggesting:
- ISC cluster preemptions (expected with "interruptible" mode)
- Checkpointing across experiments not working as intended
- Each `isc train` submission creates a NEW experiment with fresh directory

**Current behavior**:
```
Run 1: exp-heliotrope-leeward-fish-251229 ‚Üí trains epochs 1-4 ‚Üí fails
Run 2: exp-elite-guttural-bubbler-251230 ‚Üí starts at epoch 1 (not 5!) ‚Üí fails
Run 3: exp-coherent-kind-character-251230 ‚Üí starts at epoch 1 (not continuing) ‚Üí fails
```

**Expected behavior with `--checkpoint-path`**:
```
Run 1: exp-A ‚Üí trains epochs 1-4 ‚Üí saves checkpoint
Run 2: exp-B ‚Üí loads checkpoint from exp-A ‚Üí trains epochs 5-7 ‚Üí saves checkpoint
Run 3: exp-C ‚Üí loads checkpoint from exp-B ‚Üí trains epochs 8-10 ‚Üí complete!
```

### 4. Checkpoint Structure üìÅ

**ISC AtomicDirectory Structure**:
```
/shared/artifacts/exp-NAME/checkpoints/
‚îú‚îÄ‚îÄ AtomicDirectory.latest_checkpoint ‚Üí /mnt/checkpoints/AtomicDirectory_checkpoint_N (broken symlink)
‚îú‚îÄ‚îÄ AtomicDirectory_checkpoint_N/
‚îÇ   ‚îî‚îÄ‚îÄ best_model.pt
‚îî‚îÄ‚îÄ chunks.artifactjson
```

**Checkpoint contains**:
- `epoch`: Actual epoch number (NOT AtomicDirectory number)
- `model_state_dict`: Model weights
- `optimizer_state_dict`: Optimizer state
- `scheduler_state_dict`: Learning rate scheduler state
- `metrics`: Validation metrics (F1, accuracy, loss)
- Sampler states (for distributed training resumption)

---

## Recommendations

### Short Term

1. **Let current run complete**: If a run is in progress, let it finish to see full 10-epoch results

2. **Monitor for overfitting**: Consider early stopping around epoch 5-6 based on current trends

3. **Update checkpoint path**: After each run, update the `.isc` file to point to the latest checkpoint:
   ```python
   --checkpoint-path /shared/artifacts/exp-LATEST/checkpoints/AtomicDirectory_checkpoint_N/best_model.pt
   ```

### Medium Term

1. **Add regularization** to reduce overfitting:
   - Increase dropout rate (currently inherited from pretrained model)
   - Add weight decay to optimizer
   - Implement label smoothing in loss function

2. **Tune learning rate**: Current LR (1e-4) may be too high, causing instability
   - Try 5e-5 or 1e-5
   - Use learning rate warmup

3. **Automatic checkpoint discovery**: Modify training script to automatically find and resume from the latest checkpoint across experiments

### Long Term

1. **Evaluate on test set**: Once training completes, evaluate on the held-out test set

2. **Analyze per-rank performance**: Break down F1 scores by taxonomic rank (phylum, class, order, family, genus, species) to identify which levels are learning well

3. **Compare to baseline**: Compare multi-task model to single-task species-only model to quantify benefit of hierarchical learning

---

## Training Configuration

**Model**: BEiT base (patch size 16, 224x224 input)
- Pretrained on: ImageNet-1K + FungiTastic (species only)
- Parameters: 87.9M
- Architecture: Vision Transformer

**Dataset**: FungiTastic Full (300px resolution)
- Training samples: ~433K
- Validation samples: Variable (ClosedSet split)
- Classes: 2786 species + hierarchical taxonomy (6 ranks)

**Training Setup**:
- GPUs: 16 (interruptible mode)
- Batch size: 16 per GPU (256 global)
- Learning rate: 1e-4
- Optimizer: AdamW (inferred from checkpoint)
- Scheduler: Cosine annealing (inferred)
- Loss: Weighted Cross-Entropy with class balancing
- Epochs: 10 (target)

**Multi-task Heads**:
1. Phylum (10 classes)
2. Class (42 classes)
3. Order (173 classes)
4. Family (534 classes)
5. Genus (1472 classes)
6. Species (2786 classes)

---

## Next Steps

1. ‚úÖ Verify epoch accounting (COMPLETE - working correctly)
2. ‚è≥ Complete 10-epoch training run
3. ‚è≥ Analyze per-rank metrics
4. ‚è≥ Evaluate on test set
5. ‚è≥ Consider regularization improvements
6. ‚è≥ Document final results in milestone completion report

---

**Last Updated**: 2025-12-30
**Analysis Source**: `/shared/artifacts/exp-*/logs/rank_0.txt`
**Checkpoint Locations**:
- `/shared/artifacts/exp-heliotrope-leeward-fish-251229/checkpoints/AtomicDirectory_checkpoint_26/` (epoch 4)
- `/shared/artifacts/exp-coherent-kind-character-251230/checkpoints/AtomicDirectory_checkpoint_84/` (epoch 7)
- `/shared/artifacts/exp-glowing-rustic-giraffatitan-251230/checkpoints/AtomicDirectory_checkpoint_4/` (epoch 2)
